{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfbeae8b",
   "metadata": {},
   "source": [
    "# Etapas de establcer el espacio de trabajo para la importación de historicos de las criptos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89146bdb",
   "metadata": {},
   "source": [
    "## Paso 1 — Preparar proyecto y configs\n",
    "\n",
    "1) Comandos (Terminal en tu Mac)\n",
    "\n",
    "Copia y pega esto tal cual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e0a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Carpeta del proyecto y entorno virtual\n",
    "mkdir -p ~/Trading/algo-trading && cd ~/Trading/algo-trading\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "\n",
    "# 2) Estructura mínima\n",
    "mkdir -p configs services/{ingestion,processing} tools/{scripts,duckdb_queries} tests\n",
    "\n",
    "# 3) Dependencias\n",
    "cat > requirements.txt <<'EOF'\n",
    "pandas\n",
    "pyarrow\n",
    "duckdb\n",
    "httpx\n",
    "tenacity\n",
    "pydantic\n",
    "python-dateutil\n",
    "PyYAML\n",
    "pytest\n",
    "typer\n",
    "EOF\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# 4) .gitignore\n",
    "cat > .gitignore <<'EOF'\n",
    ".venv/\n",
    "__pycache__/\n",
    "*.pyc\n",
    "data/\n",
    ".env\n",
    "EOF\n",
    "\n",
    "# 5) (opcional) Crea subcarpetas dentro de TU data lake real (por si no existen)\n",
    "mkdir -p /Users/sultan/Trading/data/{checkpoints,trusted,curated}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d8063",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /Users/sultan/Trading/algo-trading\n",
    "mkdir -p configs\n",
    "\n",
    "# data_paths.yml\n",
    "cat > configs/data_paths.yml <<'YAML'\n",
    "data_root: \"/Users/sultan/Trading/data\"\n",
    "checkpoints_dir: \"/Users/sultan/Trading/data/checkpoints\"\n",
    "trusted_root: \"/Users/sultan/Trading/data/trusted\"\n",
    "curated_root: \"/Users/sultan/Trading/data/curated\"\n",
    "YAML\n",
    "\n",
    "# symbols.yml\n",
    "cat > configs/symbols.yml <<'YAML'\n",
    "market: futures\n",
    "segment: usdtm\n",
    "contract: perpetual\n",
    "intervals: [\"1d\"]\n",
    "\n",
    "symbol_map:\n",
    "  \"BTCUSDT\": \"BTCUSDT\"\n",
    "  \"ETHUSDT\": \"ETHUSDT\"\n",
    "  \"BNBUSDT\": \"BNBUSDT\"\n",
    "  \"XRPUSDT\": \"XRPUSDT\"\n",
    "  \"ADAUSDT\": \"ADAUSDT\"\n",
    "  \"DOGEUSDT\": \"DOGEUSDT\"\n",
    "  \"SOLUSDT\": \"SOLUSDT\"\n",
    "  \"TRXUSDT\": \"TRXUSDT\"\n",
    "  \"DOTUSDT\": \"DOTUSDT\"\n",
    "  \"LTCUSDT\": \"LTCUSDT\"\n",
    "  \"1000SHIBUSDT.P\": \"1000SHIBUSDT\"\n",
    "  \"BCHUSDT\": \"BCHUSDT\"\n",
    "  \"LINKUSDT\": \"LINKUSDT\"\n",
    "  \"AVAXUSDT\": \"AVAXUSDT\"\n",
    "  \"XMRUSDT.P\": \"XMRUSDT\"\n",
    "  \"XLMUSDT\": \"XLMUSDT\"\n",
    "  \"UNIUSDT\": \"UNIUSDT\"\n",
    "  \"ETCUSDT\": \"ETCUSDT\"\n",
    "  \"HBARUSDT\": \"HBARUSDT\"\n",
    "YAML\n",
    "\n",
    "# orchestration.yml\n",
    "cat > configs/orchestration.yml <<'YAML'\n",
    "jobs:\n",
    "  ingest_daily:\n",
    "    schedule_utc: \"00:05\"\n",
    "    streams: [\"klines_1d\", \"funding\", \"open_interest\", \"mark_index\"]\n",
    "retries:\n",
    "  max_attempts: 5\n",
    "  base_sleep_seconds: 1.0\n",
    "  max_sleep_seconds: 60.0\n",
    "YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Paso 2 — Test mínimo (asegura que todo está bien)\n",
    "\n",
    "tests/test_setup.py\n",
    "\n",
    "\n",
    "import yaml, pathlib\n",
    "\n",
    "def test_configs_exist_and_symbols_loaded():\n",
    "    base = pathlib.Path(__file__).resolve().parents[1]\n",
    "    for rel in [\"configs/symbols.yml\", \"configs/data_paths.yml\", \"configs/orchestration.yml\"]:\n",
    "        assert (base / rel).exists(), f\"Missing {rel}\"\n",
    "\n",
    "    with open(base / \"configs/symbols.yml\", \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    smap = cfg.get(\"symbol_map\", {})\n",
    "    assert len(smap) == 19, f\"Expected 19 mapped symbols, got {len(smap)}\"\n",
    "\n",
    "    # Normalización de sufijos .P\n",
    "    assert smap[\"1000SHIBUSDT.P\"] == \"1000SHIBUSDT\"\n",
    "    assert smap[\"XMRUSDT.P\"] == \"XMRUSDT\"\n",
    "\n",
    "def test_paths_point_to_user_data():\n",
    "    base = pathlib.Path(__file__).resolve().parents[1]\n",
    "    dp = yaml.safe_load((base / \"configs\" / \"data_paths.yml\").read_text())\n",
    "    assert dp[\"data_root\"] == \"/Users/sultan/Trading/data\"\n",
    "    assert dp[\"checkpoints_dir\"].startswith(dp[\"data_root\"])\n",
    "    assert dp[\"trusted_root\"].startswith(dp[\"data_root\"])\n",
    "    assert dp[\"curated_root\"].startswith(dp[\"data_root\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a7243",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytest -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b2c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Esperado: 2 passed\n",
    "\n",
    "Si pasa, ya quedaste con:\n",
    "• Proyecto base ✔️\n",
    "• Data Lake apuntando a tu carpeta real ✔️\n",
    "• Lista de símbolos de futuros (PERP, USDT-M, 1d) ✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e79ace",
   "metadata": {},
   "source": [
    "## Paso 2 — Cliente de exchange + pipeline (solo OHLCV 1d)\n",
    "\n",
    "1) Crea archivos y carpetas\n",
    "\n",
    "Copia y pega tal cual en tu terminal (dentro de ~/Trading/algo-trading y con el venv activo):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b10e294",
   "metadata": {},
   "source": [
    "# Carpeta destino\n",
    "cd /Users/sultan/Trading/algo-trading\n",
    "\n",
    "# Archivos fuente\n",
    "mkdir -p services/ingestion services/processing\n",
    "\n",
    "# 1) Cliente Binance (klines 1d + paginación + backoff)\n",
    "cat > services/ingestion/binance_client.py <<'PY'\n",
    "from __future__ import annotations\n",
    "import math, time\n",
    "from typing import Iterable, List, Dict, Any, Optional\n",
    "import httpx\n",
    "from tenacity import retry, wait_exponential_jitter, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "BINANCE_UF_BASE = \"https://fapi.binance.com\"  # USDT-M Futures\n",
    "\n",
    "class BinanceRateLimit(Exception): ...\n",
    "class BinanceHTTPError(Exception): ...\n",
    "\n",
    "def _to_ms(ts: float) -> int:\n",
    "    return int(ts * 1000)\n",
    "\n",
    "@retry(\n",
    "    reraise=True,\n",
    "    wait=wait_exponential_jitter(initial=1, max=60),\n",
    "    stop=stop_after_attempt(7),\n",
    "    retry=retry_if_exception_type((BinanceRateLimit, BinanceHTTPError, httpx.ConnectError, httpx.ReadTimeout))\n",
    ")\n",
    "def _get(path: str, params: Dict[str, Any]) -> Any:\n",
    "    url = BINANCE_UF_BASE + path\n",
    "    with httpx.Client(timeout=30) as client:\n",
    "        r = client.get(url, params=params)\n",
    "    if r.status_code == 429:\n",
    "        raise BinanceRateLimit(\"HTTP 429 rate limit\")\n",
    "    if r.status_code >= 400:\n",
    "        raise BinanceHTTPError(f\"http {r.status_code}: {r.text[:200]}\")\n",
    "    return r.json()\n",
    "\n",
    "def fetch_klines_1d(symbol: str, start_ms: Optional[int]=None, end_ms: Optional[int]=None, limit: int=1500) -> List[List[Any]]:\n",
    "    \"\"\"\n",
    "    Descarga klines 1d (OHLCV) para USDT-M Futures.\n",
    "    Devuelve lista de velas en el formato de Binance (listas).\n",
    "    \"\"\"\n",
    "    params = {\"symbol\": symbol, \"interval\": \"1d\", \"limit\": limit}\n",
    "    if start_ms is not None: params[\"startTime\"] = start_ms\n",
    "    if end_ms is not None: params[\"endTime\"] = end_ms\n",
    "    data = _get(\"/fapi/v1/klines\", params)\n",
    "    # Estructura por vela:\n",
    "    # [0] openTime, [1] open, [2] high, [3] low, [4] close, [5] volume,\n",
    "    # [6] closeTime, [7] quoteAssetVolume, [8] numberOfTrades,\n",
    "    # [9] takerBuyBase, [10] takerBuyQuote, [11] ignore\n",
    "    return data\n",
    "\n",
    "def paginate_klines_1d(symbol: str, start_ms: int, end_ms: int, limit: int=1500) -> Iterable[List[List[Any]]]:\n",
    "    \"\"\"\n",
    "    Generador que itera en ventanas hasta cubrir [start_ms, end_ms].\n",
    "    \"\"\"\n",
    "    current = start_ms\n",
    "    while current <= end_ms:\n",
    "        batch = fetch_klines_1d(symbol, start_ms=current, end_ms=end_ms, limit=limit)\n",
    "        if not batch:\n",
    "            break\n",
    "        yield batch\n",
    "        last_close = batch[-1][6]\n",
    "        next_start = last_close + 1\n",
    "        if next_start <= current:  # seguridad\n",
    "            next_start = current + 1\n",
    "        current = next_start\n",
    "        # pequeño respiro para ser amable con la API\n",
    "        time.sleep(0.2)\n",
    "PY\n",
    "\n",
    "# 2) Validación y normalización de OHLCV\n",
    "cat > services/processing/validate.py <<'PY'\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_klines_to_df(klines: list[list]) -> pd.DataFrame:\n",
    "    cols = [\"open_time\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"close_time\",\"quote_volume\",\"trades\",\"taker_buy_base\",\"taker_buy_quote\",\"ignore\"]\n",
    "    df = pd.DataFrame(klines, columns=cols)\n",
    "    # tipos\n",
    "    for c in [\"open\",\"high\",\"low\",\"close\",\"volume\",\"quote_volume\",\"taker_buy_base\",\"taker_buy_quote\"]:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df[\"trades\"] = pd.to_numeric(df[\"trades\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    # fecha de referencia = close_time normalizado a día UTC\n",
    "    df[\"date\"] = pd.to_datetime(df[\"close_time\"], unit=\"ms\", utc=True).dt.normalize()\n",
    "    return df[[\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"trades\",\"quote_volume\",\"close_time\"]]\n",
    "\n",
    "def validate_ohlcv_rules(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # reglas OHLCV\n",
    "    ok1 = df[\"low\"] <= df[[\"open\",\"close\",\"high\"]].min(axis=1)\n",
    "    ok2 = df[\"high\"] >= df[[\"open\",\"close\",\"low\"]].max(axis=1)\n",
    "    ok3 = (df[\"volume\"] >= 0) & (df[\"trades\"].fillna(0) >= 0)\n",
    "    df = df[ ok1 & ok2 & ok3 ]\n",
    "    # orden y de-dup por (date)\n",
    "    df = df.sort_values(\"date\").drop_duplicates(subset=[\"date\"], keep=\"last\").reset_index(drop=True)\n",
    "    return df\n",
    "PY\n",
    "\n",
    "# 3) Escritura Parquet (upsert idempotente por partición year/month)\n",
    "cat > services/ingestion/writer.py <<'PY'\n",
    "from __future__ import annotations\n",
    "import json, os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def save_checkpoint(path: str, last_close_time_ms: int) -> None:\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump({\"last_close_time\": int(last_close_time_ms)}, f)\n",
    "\n",
    "def load_checkpoint(path: str) -> int | None:\n",
    "    p = Path(path)\n",
    "    if not p.exists(): return None\n",
    "    try:\n",
    "        obj = json.loads(p.read_text())\n",
    "        return int(obj.get(\"last_close_time\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def upsert_parquet_curated(df: pd.DataFrame, curated_root: str, market:str, segment:str, contract:str, symbol:str, interval:str=\"1d\") -> int:\n",
    "    if df.empty: return 0\n",
    "    root = Path(curated_root) / f\"market={market}\" / f\"segment={segment}\" / f\"contract={contract}\" / f\"symbol={symbol}\" / f\"interval={interval}\"\n",
    "    written = 0\n",
    "    for y in sorted(df[\"date\"].dt.year.unique()):\n",
    "        df_y = df[df[\"date\"].dt.year == y]\n",
    "        for m in sorted(df_y[\"date\"].dt.month.unique()):\n",
    "            part = df_y[df_y[\"date\"].dt.month == m].copy()\n",
    "            part = part.sort_values(\"date\").drop_duplicates(subset=[\"date\"], keep=\"last\")\n",
    "            part_dir = root / f\"year={y}\" / f\"month={m:02d}\"\n",
    "            part_dir.mkdir(parents=True, exist_ok=True)\n",
    "            path = part_dir / \"data.parquet\"\n",
    "            # merge incremental: lee si existe y concatena\n",
    "            if path.exists():\n",
    "                old = pd.read_parquet(path)\n",
    "                merged = pd.concat([old, part], ignore_index=True)\n",
    "                merged = merged.sort_values(\"date\").drop_duplicates(subset=[\"date\"], keep=\"last\")\n",
    "            else:\n",
    "                merged = part\n",
    "            merged.to_parquet(path, index=False)\n",
    "            written += len(part)\n",
    "    return written\n",
    "PY\n",
    "\n",
    "# 4) Pipeline (backfill + incremental) con Typer CLI\n",
    "cat > services/ingestion/pipeline.py <<'PY'\n",
    "from __future__ import annotations\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import yaml, pandas as pd, typer\n",
    "from services.ingestion.binance_client import paginate_klines_1d\n",
    "from services.processing.validate import normalize_klines_to_df, validate_ohlcv_rules\n",
    "from services.ingestion.writer import save_checkpoint, load_checkpoint, upsert_parquet_curated\n",
    "\n",
    "app = typer.Typer(help=\"Ingesta OHLCV 1d Futuros USDT-M (PERP) → Parquet\")\n",
    "\n",
    "def _load_cfg() -> tuple[dict, dict]:\n",
    "    base = Path(__file__).resolve().parents[2]  # .../algo-trading\n",
    "    paths = yaml.safe_load((base/\"configs\"/\"data_paths.yml\").read_text())\n",
    "    symbols = yaml.safe_load((base/\"configs\"/\"symbols.yml\").read_text())\n",
    "    return paths, symbols\n",
    "\n",
    "def _ckpt_path(paths:dict, symbol:str) -> str:\n",
    "    return str(Path(paths[\"checkpoints_dir\"]) / f\"{symbol}_1d_klines.json\")\n",
    "\n",
    "@app.command()\n",
    "def backfill(symbol: str, start: str=\"2017-08-17\", end: str=None):\n",
    "    \"\"\"\n",
    "    Backfill completo para un símbolo. start/end en formato YYYY-MM-DD (UTC).\n",
    "    \"\"\"\n",
    "    paths, symbols = _load_cfg()\n",
    "    api_symbol = symbols[\"symbol_map\"].get(symbol, symbol)\n",
    "    market = symbols.get(\"market\",\"futures\"); segment=symbols.get(\"segment\",\"usdtm\"); contract=symbols.get(\"contract\",\"perpetual\")\n",
    "    start_ms = int(pd.Timestamp(start, tz=\"UTC\").timestamp()*1000)\n",
    "    end_ms = int(pd.Timestamp(end, tz=\"UTC\").timestamp()*1000) if end else int(pd.Timestamp.utcnow().timestamp()*1000)\n",
    "\n",
    "    ckpt_file = _ckpt_path(paths, symbol)\n",
    "    last = load_checkpoint(ckpt_file)\n",
    "    if last is not None and last > start_ms:\n",
    "        start_ms = last + 1  # continúa desde checkpoint\n",
    "\n",
    "    total_rows = 0; last_close_ms = None\n",
    "    for batch in paginate_klines_1d(api_symbol, start_ms, end_ms):\n",
    "        df = normalize_klines_to_df(batch)\n",
    "        df = validate_ohlcv_rules(df)\n",
    "        if df.empty: \n",
    "            continue\n",
    "        upsert_parquet_curated(df, paths[\"curated_root\"], market, segment, contract, symbol, \"1d\")\n",
    "        total_rows += len(df)\n",
    "        last_close_ms = int(df[\"close_time\"].iloc[-1])\n",
    "        save_checkpoint(ckpt_file, last_close_ms)\n",
    "\n",
    "    typer.echo(json.dumps({\"symbol\": symbol, \"rows_written\": total_rows, \"last_close_time\": last_close_ms}, indent=2))\n",
    "\n",
    "@app.command()\n",
    "def delta(symbol: str):\n",
    "    \"\"\"\n",
    "    Baja solo el delta desde el checkpoint hasta 'ahora'.\n",
    "    \"\"\"\n",
    "    return backfill(symbol=symbol, start=\"1970-01-01\", end=None)  # reutiliza lógica\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app()\n",
    "PY\n",
    "\n",
    "# 5) Testes básicos (sin tocar internet): validación + upsert\n",
    "cat > tests/test_ingest_klines_local.py <<'PY'\n",
    "import pandas as pd\n",
    "from services.processing.validate import validate_ohlcv_rules\n",
    "\n",
    "def test_validate_basic_rules():\n",
    "    df = pd.DataFrame({\n",
    "        \"date\": pd.to_datetime([\"2024-01-01\",\"2024-01-02\"], utc=True),\n",
    "        \"open\":[100,110],\"high\":[120,130],\"low\":[90,100],\"close\":[115,120],\n",
    "        \"volume\":[10,20],\"trades\":[100,200],\"quote_volume\":[1000,2000],\"close_time\":[1704067200000,1704153600000]\n",
    "    })\n",
    "    out = validate_ohlcv_rules(df)\n",
    "    assert len(out)==2\n",
    "PY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
